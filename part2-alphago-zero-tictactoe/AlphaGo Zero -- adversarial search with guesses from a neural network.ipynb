{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to play the game of Go without human input\n",
    "\n",
    "In Lecture 9, we have learned about multi-agent systems and adverserial games. A central algorithm in game playing is **MiniMax**.\n",
    "\n",
    "MiniMax essentially searches the entire space of moves possible by both players at any given time till the game reaches an end. This is how it maximizes reward while the opponent is trying to minimize it. We saw an optimization called alpha-beta pruning for MiniMax which is a great improvement but only by a factor of 2. This might help in games such as chess (an AI agent can be twice as smart on the same computer now assuming it uses a plausible heuristic).\n",
    "\n",
    "TicTacToe has $3^9 = 19683$ possible states, roughly half of them are valid states. This kind of state space can be fully searched by computers.\n",
    "\n",
    "Some games such as Go are way too complicated.\n",
    "\n",
    "By [one estimate](https://www.google.com/search?q=2081681993819799846994786333448627702865224+5388453054842563945682092741961273801537852+5648451698519643907259916015628128546089888+314427129715319317557736620397247064840935&filter=0&biw=1280&bih=627) there are:\n",
    "\n",
    "```\n",
    "2081681993819799846994786333448627702865224\n",
    "5388453054842563945682092741961273801537852\n",
    "5648451698519643907259916015628128546089888\n",
    "314427129715319317557736620397247064840935\n",
    "```\n",
    "\n",
    "States!\n",
    "\n",
    "That certainly cannot be handled by even the biggest supercomputers or an earth filled with GPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go is a [googol](https://en.wikipedia.org/wiki/Googol) ($10^{100}$) times more complex than chess. It has more states than there are atoms in the universe. It has been a formidable challenge for AI.\n",
    "\n",
    "\n",
    "In March 2016 DeepMind's AlphaGo Lee became the first program to beat a human expert -- 18-time world champion Lee Sedol. (DeepBlue beat Gary Kasparov in chess in 1997 -- super human ability in Go took 19 years longer).\n",
    "\n",
    "The key insight of this software was to use a neural network to learn the game by looking at how humans play (using plays from ameteur players on internet Go servers) it to begin with, but later by playing the program against itself.\n",
    "\n",
    "In 19 Oct 2017 DeepMind unweiled a newer, more elegant version of AlphaGo -- the AlphaGo Zero. This version learnt the game all on its own through playing millions of games with its best self. In 3 days, AlphaGo Zero trained enough to beat the first version (AlphaGo Lee) 100 games to 0! In this notebook we will look at the technique carried out in this \"training\". We also implment the algorithm on the game of Tictactoe and allow you to explore how the skill of the agent improves as it learns to play better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaGo Zero's algorithm\n",
    "\n",
    "### Background\n",
    "\n",
    "#### Monte carlo tree search (MCTS)\n",
    "\n",
    "\n",
    "![](https://nikcheerla.github.io/deeplearningschool//media/branching.jpg)\n",
    "\n",
    "One way to non-exhaustively search a large state space graph is to **pick a random sample of branches**, playing each one many times to see which ones result in a victory down the road more often. This is called Monte-carlo tree search.\n",
    "\n",
    "While playing a number of random games we can store the average reward for all plays so far from a given state. At a later time we can commit to using these probability as the \"prior\" for the random sampling of branches (i.e. higher the probability of winning, the more likely the branch will be sampled.)\n",
    "\n",
    "Below is the simple Julia code which implements a simple form of MCTS for the game of tic-tac-toe\n",
    "\n",
    "- the `randomplay` function runs a random game from beginning to end\n",
    "- the `mcts_priors` runs randomplay `n` times and returns a dictionary with the average reward for each state it has explored thus far\n",
    "- the `mcts_move` takes this dictionary, a state and a player and returns a next move (by picking the one that takes you to the state with the highest reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mcts_move (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function randomplay(state, player)\n",
    "    w = winner(state)\n",
    "    if w == E && all(!iszero, state)\n",
    "        return 0, [state] # draw\n",
    "    elseif w != E\n",
    "        return (w == X ? 1 : -1), [state]\n",
    "    end\n",
    "    succ = next_states(state, player)\n",
    "    score, states = randomplay(rand(succ)[2], player == X ? O : X)\n",
    "    return score, vcat([state], states)\n",
    "end\n",
    "\n",
    "function mcts_priors(n, f=randomplay)\n",
    "    stats = Dict{State, Tuple{Int, Int}}() # we store sum and number of times chosen\n",
    "    for i=1:n\n",
    "        score, states = f(start_state, X) # run a random play -- gives score and a state\n",
    "        # for each state we keep a sum of scores and the\n",
    "        # number of times the state was taken\n",
    "        for s in states[2:end] # leave out start state\n",
    "            sum, count = haskey(stats, s) ? stats[s] : (0, 0)\n",
    "            sum += score\n",
    "            count += 1\n",
    "            stats[s] = (sum, count)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    Dict(state => stat[1] / stat[2] for (state, stat) in stats)\n",
    "end\n",
    "\n",
    "function mcts_move(state, player, priors)\n",
    "    succ = next_states(state, player)\n",
    "    (move, p), idx = findmax([(s[1], get(priors, s[2], 0)) for s in succ])\n",
    "    move\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement learning (specifically Q-learning)\n",
    "\n",
    "An alternative to tree search reinforcement learning. In this technique, the agent learns to deal with an environment (like a game) by exploring it. The agent starts out acting randomly but observes what reward its action beget and over time figures out which actions result in good rewards.\n",
    "\n",
    "A specific type of reinforcement learning is Q-learning. It consists of storing a table of rewards obtained by doing a certain action at a certain state. i.e. $Q : S \\times A \\rightarrow \\mathbb{R}$\n",
    "\n",
    "The **Q-learning** algorithm works as follows:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*QeoQEqWYYPs1P8yUwyaJVQ.png)\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/2000/0*voKUaGu68-cDuncy.)\n",
    "\n",
    "\n",
    "Hence the Q function here needs to store $|S|\\times|A|$ number of values in order to work.\n",
    "\n",
    "Q-learning will work in training an agent, for example to play games like tic-tac-toe, the $Q$ table will be of size $3^9 = 19683$ rows. (each cell can have 3 states, one of the states is the action.)\n",
    "\n",
    "\n",
    "#### An approximate Q-function\n",
    "\n",
    "The game of Go has many more states, hence it is infeasible to create a full Q-table for the game. It seems we have circled back at the same problems as search. But there is a clever trick that may let us get away with storing much less data -- **approximate** the Q-table. A modern trendy approach to approximating non-linear functions like $Q$ is to train a neural network to approximate this.\n",
    "\n",
    "More background about neural netwoks may be outside the scope of this document. Later on in the notebook we do not allude to any particular such details. It suffices to know that neural networks are black-boxes that approximate any function using a fixed amount of memory. And deep neural networks maybe thought of, approximately, as many such boxes chained together. You can however find some specifications of the type of neural networks we used down in the appendix.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*yg9-6oLTAX0sVeLBJudexA.png)\n",
    "_A neural network that approximates a Q function_\n",
    "\n",
    "  - Start off with a random neural network\n",
    "  - Keep playing the game to improve the $Q$ function it approximates\n",
    "  \n",
    "Reinforcement learning with deep neural networks (we will call this **Deep Q-learning**) have been used to play a [host of Atari games (pdf)](https://arxiv.org/pdf/1312.5602v1.pdf).\n",
    "This is not exactly what AlphaGo zero uses, but it can be used to play a lot of games including Breakout.\n",
    "\n",
    "Deep Q-learning is not exactly what is used by the AlphaGo player. It however incorporates two elements from the strategy of deep Q-learning.\n",
    "\n",
    "1. Reinforcement learning (learning by actually playing)\n",
    "2. Use of deep neural networks\n",
    "\n",
    "\n",
    "**Approximating a function that guides MCTS**\n",
    "\n",
    "AlphaGo zero creates a neural network $f_{\\theta}$ which approximates the probability distribution among all branches for the next move. Then the sampling algorithm draws from this probability distribution. It also uses the same neural network to guess the expected probability of winning from the state it is applied to.\n",
    "\n",
    "\n",
    "$$(\\textbf{p}, v) = f_{\\theta}(s)$$\n",
    "\n",
    "Here $\\textbf{p}$ maps every action available at state $s$ to a probability value that it should be sampled.\n",
    "\n",
    "\n",
    "**Self-play**\n",
    "\n",
    "A neural network needs to be told which are good moves and which are not in order to train itself to be better at the game. AlphaGo Zero keeps improving by playing against itself. Winning the game gets a reward of 1, losing gets a reward of -1.\n",
    "\n",
    "This is the simple algorithm that runs all of AlphaGo Zero!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes about implementation\n",
    "\n",
    "For the purpose of this presentation we chose [AlphaGo.jl](https://github.com/tejank10/AlphaGo.jl) a Julia implimentation of AlphaGo Zero algorithm. The package not only implements the algorithm for Go, but also for the game of [Gomoku](https://en.wikipedia.org/wiki/Gomoku) which is a **generalization of Tic-tac-toe** played on the Go board. The program allows you to tweak the size of the board and the number of squares that need to be taken up consequitively to win the game. Hence Tic Tac Toe is a special case of Gomoku when the board is of size 3x3 and you need 3 colinear squares to win.\n",
    "\n",
    "See appendix for more about the structure of the neural networks we tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting up Julia packages\n",
    "\n",
    "The following cell should install all the dependencies to run this notebook. Ignore any build errors to do with CUDAnative and CUDAdrv packages, these are not required but are installed by default by the AlphaGo implementation.\n",
    "\n",
    "You will have to restart the Jupyter notebook server and start this notebook again to get some of the UI packages to work (this will let you play Tic tac toe against agents of different skill levels :-) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"env\") # like virtualenv we have some package state files here\n",
    "Pkg.resolve()       # installs all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load saved models\n",
    "\n",
    "using AlphaGo, Flux\n",
    "using BSON: @load\n",
    "\n",
    "function load_model(str, n, env::AlphaGo.GameEnv)\n",
    "  @load str*\"/agz_base-$n.bson\" bn\n",
    "  @load str*\"/agz_value-$n.bson\" value\n",
    "  @load str*\"/agz_policy-$n.bson\" policy\n",
    "\n",
    "  @load str*\"/weights/agz_base-$n.bson\" bn_weights\n",
    "  @load str*\"/weights/agz_value-$n.bson\" val_weights\n",
    "  @load str*\"/weights/agz_policy-$n.bson\" pol_weights\n",
    "\n",
    "  Flux.loadparams!(bn, bn_weights)\n",
    "  Flux.loadparams!(value, val_weights)\n",
    "  Flux.loadparams!(policy, pol_weights)\n",
    "\n",
    "  NeuralNet(env; base_net=bn, value=value, policy=policy)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/shashi/.julia/compiled/v1.0/WebIO/v9IED.ji for WebIO [0f1e0344-ec1d-5b48-a673-e5cf874b6c29]\n",
      "└ @ Base loading.jl:1187\n",
      "WARNING: Method definition #sort(Any, typeof(Base.sort), Base.Dict{K, V} where V where K) in module OrderedCollections overwritten in module DataStructures.\n",
      "WARNING: Method definition sort(Base.Dict{K, V} where V where K) in module OrderedCollections at /home/shashi/.julia/packages/OrderedCollections/Pr9Pa/src/dict_sorting.jl:21 overwritten in module DataStructures at /home/shashi/.julia/dev/DataStructures/src/dict_sorting.jl:21.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "    <script class='js-collapse-script'>\n",
       "        var curMatch =\n",
       "            window.location.href\n",
       "            .match(/(.*?)\\/notebooks\\/.*\\.ipynb/);\n",
       "\n",
       "        curMatch = curMatch ||\n",
       "            window.location.href\n",
       "            .match(/(.*?)\\/apps\\/.*\\.ipynb/);\n",
       "\n",
       "        if ( curMatch ) {\n",
       "            $('head').append('<base href=\"' + curMatch[1] + '/\">');\n",
       "        }\n",
       "    </script>\n"
      ],
      "text/plain": [
       "HTML{String}(\"    <script class='js-collapse-script'>\\n        var curMatch =\\n            window.location.href\\n            .match(/(.*?)\\\\/notebooks\\\\/.*\\\\.ipynb/);\\n\\n        curMatch = curMatch ||\\n            window.location.href\\n            .match(/(.*?)\\\\/apps\\\\/.*\\\\.ipynb/);\\n\\n        if ( curMatch ) {\\n            \\$('head').append('<base href=\\\"' + curMatch[1] + '/\\\">');\\n        }\\n    </script>\\n\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script class='js-collapse-script' src='/assetserver/e8f953a4abf5e4095b83e225fc5e11254819fa44-assets/webio/dist/bundle.js'></script>"
      ],
      "text/plain": [
       "HTML{String}(\"<script class='js-collapse-script' src='/assetserver/e8f953a4abf5e4095b83e225fc5e11254819fa44-assets/webio/dist/bundle.js'></script>\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script class='js-collapse-script' src='/assetserver/e8f953a4abf5e4095b83e225fc5e11254819fa44-assets/providers/ijulia_setup.js'></script>"
      ],
      "text/plain": [
       "HTML{String}(\"<script class='js-collapse-script' src='/assetserver/e8f953a4abf5e4095b83e225fc5e11254819fa44-assets/providers/ijulia_setup.js'></script>\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "  <script class='js-collapse-script'>\n",
       "    $('.js-collapse-script').parent('.output_subarea').css('padding', '0');\n",
       "  </script>\n"
      ],
      "text/plain": [
       "HTML{String}(\"  <script class='js-collapse-script'>\\n    \\$('.js-collapse-script').parent('.output_subarea').css('padding', '0');\\n  </script>\\n\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/shashi/.julia/compiled/v1.0/InteractBase/63c8J.ji for InteractBase [d3863d7c-f0c8-5437-a7b4-3ae773c01009]\n",
      "└ @ Base loading.jl:1187\n",
      "WARNING: Method definition sort(Base.Dict{K, V} where V where K) in module DataStructures at /home/shashi/.julia/dev/DataStructures/src/dict_sorting.jl:21 overwritten in module OrderedCollections at /home/shashi/.julia/packages/OrderedCollections/Pr9Pa/src/dict_sorting.jl:21.\n",
      "WARNING: Method definition #sort(Any, typeof(Base.sort), Base.Dict{K, V} where V where K) in module DataStructures overwritten in module OrderedCollections.\n",
      "WARNING: Method definition sort(Base.Dict{K, V} where V where K) in module DataStructures at /home/shashi/.julia/dev/DataStructures/src/dict_sorting.jl:21 overwritten in module OrderedCollections at /home/shashi/.julia/packages/OrderedCollections/Pr9Pa/src/dict_sorting.jl:21.\n",
      "WARNING: Method definition #sort(Any, typeof(Base.sort), Base.Dict{K, V} where V where K) in module DataStructures overwritten in module OrderedCollections.\n",
      "WARNING: Method definition sort(Base.Dict{K, V} where V where K) in module DataStructures at /home/shashi/.julia/dev/DataStructures/src/dict_sorting.jl:21 overwritten in module OrderedCollections at /home/shashi/.julia/packages/OrderedCollections/Pr9Pa/src/dict_sorting.jl:21.\n",
      "WARNING: Method definition #sort(Any, typeof(Base.sort), Base.Dict{K, V} where V where K) in module DataStructures overwritten in module OrderedCollections.\n",
      "WARNING: Method definition sort(Base.Dict{K, V} where V where K) in module DataStructures at /home/shashi/.julia/dev/DataStructures/src/dict_sorting.jl:21 overwritten in module OrderedCollections at /home/shashi/.julia/packages/OrderedCollections/Pr9Pa/src/dict_sorting.jl:21.\n",
      "WARNING: Method definition #sort(Any, typeof(Base.sort), Base.Dict{K, V} where V where K) in module DataStructures overwritten in module OrderedCollections.\n",
      "┌ Info: Recompiling stale cache file /home/shashi/.julia/compiled/v1.0/AlphaGo/hIRw1.ji for AlphaGo [da2c498e-f8db-5b64-9cc4-315cb361752b]\n",
      "└ @ Base loading.jl:1187\n",
      "WARNING: Method definition #sort(Any, typeof(Base.sort), Base.Dict{K, V} where V where K) in module OrderedCollections overwritten in module DataStructures.\n",
      "WARNING: Method definition sort(Base.Dict{K, V} where V where K) in module OrderedCollections at /home/shashi/.julia/packages/OrderedCollections/Pr9Pa/src/dict_sorting.jl:21 overwritten in module DataStructures at /home/shashi/.julia/dev/DataStructures/src/dict_sorting.jl:21.\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: AlphaGo not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: AlphaGo not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at none:0",
      " [2] include at ./boot.jl:317 [inlined]",
      " [3] include_relative(::Module, ::String) at ./loading.jl:1041",
      " [4] include(::Module, ::String) at ./sysimg.jl:29",
      " [5] include(::String) at ./client.jl:388",
      " [6] top-level scope at In[1]:1"
     ]
    }
   ],
   "source": [
    "include(\"ui.jl\") # The game-playing UI code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing against the neural network\n",
    "\n",
    "Below is a UI where you can play against the neural network at various stages of its learning.\n",
    "\n",
    "In the cell immediately below, the neural network has been only trained on 10 games.\n",
    "\n",
    "The agent is smart to win or draw most games already, but try playing (1,1), (3,3), (1, 3), (1, 2) -- you will win this game. (note: $(i,j)$ is $i^{th}$ row $j^{th}$ column.)\n",
    "\n",
    "In the subsequent cells you can adjust the level of training the Neural network has had and play the same game to see where the Neural network figured out to draw this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# play with the dumbest network first:\n",
    "tictactoe = AlphaGo.GomokuEnv(3,3) # the specific case which is tictactoe\n",
    "\n",
    "# there are 50 levels we have trained, second argument is the level\n",
    "nn = load_model(\"models_48_81boot\", \"1\", tictactoe)\n",
    "\n",
    "\n",
    "# play_with lets you play the game with any given neural network\n",
    "b,t = play_with(tictactoe, nn)\n",
    "b # -- this is the board, and gets interactively displayed\n",
    "\n",
    "# NOTE: once you click on a square it may take several seconds for the first move to be registered\n",
    "# Julia is a just-in-time compiled language, the first move compiles the whole\n",
    "# program to C-like machine code, hence be patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with an agent which has various levels of training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@manipulate for level = slider(1:50, label=\"Experience level (10 games each): \", value=1)\n",
    "    gametype = AlphaGo.GomokuEnv(3,3) # the specific case which is tictactoe\n",
    "    nn = load_model(\"models_48_81boot\", string(level), gametype) # there are 50 levels we have trained, second argument is the level\n",
    "\n",
    "    play_with(gametype, nn)[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"optimal.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison against optimal player over time\n",
    "\n",
    "The true test of skill for the neural network is comparison against the optimal policy for the game. Fortunately, in the game of Tic tac toe we can actually have do this. We have written an optimal player in `optimal.jl` file, it is in the same vein as the one in Homework 6. If you are curious about how the Julia code looks, you may have a look in there. We also have a function play_optimal which will play against a given neural network and return the result 1 if the optimal player wins 0 if a draw and -1 if the neural network wins (this never happens, since if all players played optimally there can only ever be a draw).\n",
    "\n",
    "If you play every level of the NN player against the optimal player many times, you can plot what percentage of games a given level of player can draw or win, the plot hence shows the effectiveness of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "addprocs(4) # add 4 more Julia processors\n",
    "using Distributed\n",
    "\n",
    "stats = pmap(1:50) do i # run the evaluation in parallel\n",
    "        gametype = AlphaGo.GomokuEnv(3,3) # the specific case which is tictactoe\n",
    "        nn = load_model(\"models_48_81boot\", string(i), gametype)\n",
    "\n",
    "        plays = [play_optimal(gametype, nn) for i=1:20]\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "using Gadfly\n",
    "Gadfly.plot(y=(20 .- sum.(stats)) / 20 * 100, x=[1:50;] .* 10,\n",
    "    Geom.point, Geom.smooth, Guide.xlabel(\"Games trained on \"), Guide.ylabel(\"NN vs. Optimal (% not lost)\"))\n",
    "```\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/25916/47750325-8f5f5f80-dc65-11e8-9a23-6dd0475d238a.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "We initially trained the network on 10000 games without considering the option `start_training_after` which is by default set to 50000 -- these are the number of sample branches explored. So our training did not start to learn anything until it played about 6000 games, but was sampling games to assign priors for the monte carlo tree search. However, once the training started the priors accumulated from the 6000 games were enough to quickly learn the optimal strategy. The plot below shows this.\n",
    "\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/25916/47750905-cc782180-dc66-11e8-8ac8-b38a4154f26b.png\" width=\"600\">\n",
    "\n",
    "However for the purpose of the examples here we went with the much lesser trained network of a setup where we only use 81 initial games to set the prior for the monte-carlo sampling. Also we only trained on 500 games and captured a snaptshot of the agent more frequently. This makes for a much more human-accessible example. You can actually play against the agent and see how dumb it started out and how it learned specific moves.\n",
    "\n",
    "\n",
    "### Don Knuth takes a crack at TicTacToe in 10000 bytes\n",
    "\n",
    "Here is Donald Knuth talking about solving the game of tic tac toe by training against an optimal player https://www.youtube.com/watch?v=_c3dKYrjj2Q . The idea of learning from self-play in AlphaGo are certainly not new, but the cleverness is in the monte carlo tree search. And of course the enormous amount of compute power required to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- **Mastering the game of Go without human knowledge**  -- Nature (Oct. 2017)\n",
    "David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel & Demis Hassabis\n",
    "\n",
    "- **AlphaGo Zero Explained - On AI** blog post https://nikcheerla.github.io/deeplearningschool/2018/01/01/AlphaZero-Explained/\n",
    "\n",
    "- **AlphaGo.jl** - Julia implementation of the AlphaGo Zero problem on Go and Gomoku -- a generalization of tic-tac-toe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2-pre",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
